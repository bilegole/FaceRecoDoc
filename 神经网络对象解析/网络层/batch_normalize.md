#### 为什么要插入BatchNorm2d
在我们训练神经网络的时候,后面层的输入完全依赖于前面层的输出,因此经常会出现这样一个问题:
后面的网络已经学习了前面网络的给出的特征,但随着前面层的变动,可能会发生比例的变化,比如原来输入的是(0,1,0),重新训练后变成了(0,10,0).显而易见的,后面的层将会需要再次修正自己以适应前面层的变化.
假如添加BatchNormalize层,则可以在不支付额外训练参数代价的同时,保留已学习到的特征.

---
# 相关链接
- [[BatchNorm2d]]